{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "cuda = torch.device('cuda')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to find files\n",
    "PATH_TO_FOLDER_WITH_DATASETS = \"/home/lorenzo/datasets/new_dataset/training_data/\"\n",
    "MODEL_SAVE_FOLDER = \"/home/lorenzo/models/fast_tunnel_traversal\"\n",
    "MODEL_NAME = \"dir_to_axis_data_aug_larger_dataset_vld\"\n",
    "# Parameters to choose the dataset\n",
    "DATASET_PARAMETERS = {\n",
    "    \"conversor/img_size\":100,\n",
    "    \"conversor/max_coord_val\":10,\n",
    "    \"label_distance\":5,\n",
    "    \"dataset_type\":\"dir_to_axis\",\n",
    "    \"number_of_samples_per_env\":10000,\n",
    "    }\n",
    "# Network Parameters\n",
    "MODULE_TO_IMPORT_NETWORK = \"tunnel_yaw_prediction.models\"\n",
    "NETWORK_CLASS_NAME = \"TunnelYawPredictor\"\n",
    "N_EPOCHS = 64\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.001\n",
    "DO_DATA_AUG = False\n",
    "# Derived paramters\n",
    "PATH_TO_MODEL = os.path.join(MODEL_SAVE_FOLDER, MODEL_NAME+\".torch\")\n",
    "PATH_TO_MODEL_INFO = os.path.join(MODEL_SAVE_FOLDER, MODEL_NAME+\".json\")\n",
    "MODULE = importlib.import_module(MODULE_TO_IMPORT_NETWORK)\n",
    "MODEL = getattr(MODULE,NETWORK_CLASS_NAME)\n",
    "os.makedirs(MODEL_SAVE_FOLDER,exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check datasets available in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the desired dataset by number\n",
      "0: 2023-07-14_15:32:29 // info: {'name': '2023-07-14_15:32:29', 'dataset_type': 'dir_to_axis', 'data_folder': '/home/lorenzo/datasets/new_dataset', 'number_of_samples_per_env': 10000, 'max_rel_yaw_deg': 45, 'label_distance': 5, 'max_horizontal_displacement': 1.8, 'min_vertical_displacement': -0.1, 'max_vertical_displacement': 0.1, 'max_inclination_deg': 10, 'robot_name': '/', 'image_topic': '/cenital_image', 'conversor/max_coord_val': 10, 'conversor/img_size': 100}\n",
      "\t -> n_samples: 119038\n",
      "1: 2023-07-17_13:30:17 // info: {'name': '2023-07-17_13:30:17', 'dataset_type': 'dir_to_axis', 'data_folder': '/home/lorenzo/datasets/new_dataset', 'number_of_samples_per_env': 10000, 'max_rel_yaw_deg': 45, 'label_distance': 5, 'max_horizontal_displacement': 1.8, 'min_vertical_displacement': -0.1, 'max_vertical_displacement': 0.1, 'max_inclination_deg': 10, 'robot_name': '/', 'image_topic': '/cenital_image', 'conversor/max_coord_val': 10, 'conversor/img_size': 100}\n",
      "\t -> n_samples: 360000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset at /home/lorenzo/datasets/new_dataset/training_data/2023-07-17_13:30:17\n"
     ]
    }
   ],
   "source": [
    "matching_datasets = []\n",
    "for dataset_name in os.listdir(PATH_TO_FOLDER_WITH_DATASETS):\n",
    "    path_to_dataset = os.path.join(PATH_TO_FOLDER_WITH_DATASETS,dataset_name)\n",
    "    dataset_info_file_path = os.path.join(path_to_dataset, \"info.json\")\n",
    "    with open(dataset_info_file_path, \"r\") as f:\n",
    "        dataset_info = json.load(f)\n",
    "    matching_datasets.append(dataset_name)\n",
    "    for dataset_param in DATASET_PARAMETERS:\n",
    "        if not dataset_param in dataset_info:\n",
    "            matching_datasets.remove(dataset_name)\n",
    "            break\n",
    "        elif dataset_info[dataset_param]!= DATASET_PARAMETERS[dataset_param]:\n",
    "            matching_datasets.remove(dataset_name)\n",
    "            break\n",
    "if len(matching_datasets) == 1:\n",
    "    path_to_dataset = os.path.join(PATH_TO_FOLDER_WITH_DATASETS,matching_datasets[0])\n",
    "    path_to_dataset_info = os.path.join(path_to_dataset, \"info.json\")\n",
    "    with open(path_to_dataset_info, \"r\") as f:\n",
    "        dataset_info = json.load(f)\n",
    "elif len(matching_datasets) == 0:\n",
    "    raise Exception(\"No dataset matches the params\")\n",
    "else:\n",
    "    print(f\"Choose the desired dataset by number\")\n",
    "    for i, dataset_name in enumerate(matching_datasets):\n",
    "        path_to_dataset_info = os.path.join(PATH_TO_FOLDER_WITH_DATASETS, dataset_name, \"info.json\")\n",
    "        with open(path_to_dataset_info, \"r\") as f:\n",
    "            dataset_info = json.load(f)\n",
    "        print(f\"{i}: {dataset_name} // info: {dataset_info}\")\n",
    "        print(f\"\\t -> n_samples: {len(os.listdir(os.path.join(PATH_TO_FOLDER_WITH_DATASETS, dataset_name)))-1}\")\n",
    "    n = input(\"Number of selected dataset: \")\n",
    "    n = int(n)\n",
    "    path_to_dataset = os.path.join(PATH_TO_FOLDER_WITH_DATASETS, matching_datasets[n])\n",
    "    path_to_dataset_info = os.path.join(path_to_dataset, \"info.json\")\n",
    "    with open(path_to_dataset_info, \"r\") as f:\n",
    "        dataset_info = json.load(f)\n",
    "print(f\"Loaded dataset at {path_to_dataset:}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360000/360000 [05:12<00:00, 1152.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path_to_dataset, dataset_info):\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\")\n",
    "        self.load_dataset(path_to_dataset, dataset_info)\n",
    "        self.data_augmentation=torch.nn.Sequential(\n",
    "            transforms.RandomErasing()\n",
    "        )\n",
    "    def load_dataset(self, path_to_dataset, dataset_info):\n",
    "        elements_in_dataset = os.listdir(path_to_dataset)\n",
    "        elements_in_dataset.remove(\"info.json\")\n",
    "        elements_in_dataset.sort()\n",
    "        self.n_datapoints = len(elements_in_dataset)\n",
    "        self.labels = torch.zeros((self.n_datapoints, 1)).float()\n",
    "        self.images = torch.zeros((self.n_datapoints,1,100,100)).float()\n",
    "        folder_loop = tqdm(elements_in_dataset,)\n",
    "        for dtp_n, dtp_name in enumerate(folder_loop):\n",
    "            dtp_path = os.path.join(path_to_dataset, dtp_name)\n",
    "            dtp = np.load(dtp_path)\n",
    "            self.labels[dtp_n, :] = torch.tensor(dtp[\"label\"])\n",
    "            self.images[dtp_n,0, :,:] = torch.tensor(dtp[\"image\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_datapoints\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx, ...]\n",
    "        result = self.labels[idx, ...]\n",
    "        return image.float(), result.float()\n",
    "    def delete(self, idx):\n",
    "        self.images = torch.cat(self.images[0:idx,...],self.images[idx+1,:])\n",
    "        self.labels= torch.cat(self.labels[0:idx,...],self.labels[idx+1,:])\n",
    "\n",
    "dataset = ImageDataset(path_to_dataset, dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset,[0.9,0.1])\n",
    "torch.cuda.empty_cache()\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=5)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_circle_to_img(image:np.ndarray):\n",
    "    height, width = image.shape\n",
    "    radius = np.random.randint(1, 10)\n",
    "    thickness = np.random.randint(1,3)\n",
    "    center_x = np.random.randint(0,width)\n",
    "    center_y = np.random.randint(0,height)\n",
    "    return cv2.circle(image, (center_x, center_y),radius, 1, thickness )\n",
    "\n",
    "def add_random_line_to_img(image:np.ndarray):\n",
    "    height, width = image.shape\n",
    "    thickness = np.random.randint(1,3)\n",
    "    p1_x = np.random.randint(0,width)\n",
    "    p1_y = np.random.randint(0,height)\n",
    "    p2_x = np.random.randint(0,width)\n",
    "    p2_y = np.random.randint(0,height)\n",
    "    return cv2.line(image, (p1_x, p1_y), (p2_x, p2_y), 1, thickness)\n",
    "\n",
    "def add_dot_to_img(image:np.ndarray):\n",
    "    height, width = image.shape\n",
    "    center_x = np.random.randint(0,width)\n",
    "    center_y = np.random.randint(0,height)\n",
    "    return cv2.circle(image, (center_x, center_y),1, 1, 1)\n",
    "\n",
    "def data_augment_numpy_image(image:np.ndarray):\n",
    "    for _ in range(2):\n",
    "        if np.random.random() >0.5:\n",
    "            image[0,...] = add_random_circle_to_img(image[0,...])\n",
    "        if np.random.random() >0.5:\n",
    "            image[0,...] = add_random_line_to_img(image[0,...])\n",
    "    for _ in range(30):\n",
    "        if np.random.random() > 0.5:\n",
    "            image[0,...] = add_dot_to_img(image[0,...])\n",
    "    return image\n",
    "def data_augment_torch_batch(torch_batch:torch.Tensor):\n",
    "    numpy_batch = torch_batch.detach().numpy()   \n",
    "    for i in range(len(numpy_batch)):\n",
    "        numpy_batch[i, ...] = data_augment_numpy_image(numpy_batch[i,...])\n",
    "    augmented_torch_batch = torch.tensor(numpy_batch)\n",
    "    return augmented_torch_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_train(network:nn.Module, train_loader, test_loader, criterion, optimizer, n_epochs,lr,name,tensorborad_folder=\"/home/lorenzo/tensor_board\",):\n",
    "    writer = SummaryWriter(log_dir=tensorborad_folder)\n",
    "    n_iter = 0\n",
    "    for n_epoch,epoch in enumerate(tqdm(range(n_epochs))):  # loop over the dataset multiple times\n",
    "        network.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            torch.cuda.empty_cache()\n",
    "            inputs, labels = data\n",
    "            if DO_DATA_AUG: \n",
    "                inputs = data_augment_torch_batch(inputs)\n",
    "            inputs = inputs.to(torch.device(\"cuda\"))\n",
    "            labels = labels.to(torch.device(\"cuda\"))\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(f\"every_train_loss\",loss,n_iter)\n",
    "            n_iter+=1\n",
    "        writer.add_scalar(f\"train_loss_per_epoch\",loss,n_epoch)\n",
    "        network.eval() \n",
    "        test_losses = np.zeros((0))\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(torch.device(\"cuda\"))\n",
    "            labels = labels.to(torch.device(\"cuda\"))\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.cpu().detach().numpy()\n",
    "            test_losses = np.concatenate([test_losses, np.reshape(loss, -1)])\n",
    "        test_loss = np.mean(test_losses)\n",
    "        writer.add_scalar(f\"test_loss_per_epoch\",test_loss,n_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(\"/home/lorenzo/models/gallery_detection/procedural_datasets/dataset_03/gallery_detector_v3-_r10_lr002_3.torch\", \"rb\") as f:\n",
    "        network.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100000\n",
      "0.065000\n",
      "0.042250\n",
      "0.027463\n",
      "0.017851\n",
      "0.011603\n",
      "0.007542\n",
      "0.004902\n",
      "0.003186\n",
      "0.002071\n",
      "0.001346\n",
      "0.000875\n",
      "0.000569\n",
      "0.000370\n",
      "0.000240\n",
      "0.000156\n",
      "0.000102\n",
      "0.000066\n",
      "0.000043\n",
      "0.000028\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.1*0.65**n for n in range(20)]\n",
    "for i in lrs:\n",
    "    print(f\"{i:05f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [1:00:39<00:00, 56.86s/it]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "network = MODEL().to(cuda).float()\n",
    "torch.cuda.empty_cache()\n",
    "optimizer = torch.optim.Adam(\n",
    "    network.parameters(),\n",
    "    lr=LR,\n",
    ")\n",
    "loss_hist = basic_train(\n",
    "    network, train_dataloader,test_dataloader, criterion, optimizer, N_EPOCHS, LR,MODEL_NAME\n",
    ")\n",
    "network.eval()\n",
    "network.to(\"cpu\")\n",
    "# Calculate the error on the validation data\n",
    "mean_squared_errors = np.zeros(0)\n",
    "for test_data in test_dataloader:\n",
    "    inputs, labels = test_data\n",
    "    outputs = network(inputs)\n",
    "    mean_squared_errors = np.hstack([mean_squared_errors,criterion(outputs, labels).detach().numpy()])\n",
    "mean_squared_error = np.mean(mean_squared_errors)\n",
    "mean_error_rad = np.sqrt(mean_squared_error)\n",
    "mean_error_deg = np.rad2deg(mean_error_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4249089162462485\n"
     ]
    }
   ],
   "source": [
    "print(mean_error_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"/home/lorenzo/models/fast_tunnel_traversal/dir_to_axis_data_aug_larger_dataset_vld_2.torch\"\n",
    "PATH_TO_MODEL_INFO = \"/home/lorenzo/models/fast_tunnel_traversal/dir_to_axis_data_aug_larger_dataset_vld_2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/models/fast_tunnel_traversal/dir_to_axis_data_aug_larger_dataset_vld_2.torch\n"
     ]
    }
   ],
   "source": [
    "info_about_training = {\n",
    "    \"path_to_dataset\": path_to_dataset,\n",
    "    \"path_to_dataset_info\":path_to_dataset_info,\n",
    "    \"model_name\":MODEL_NAME,\n",
    "    \"dataset_paramters\":DATASET_PARAMETERS,\n",
    "    \"module_to_import_network\":MODULE_TO_IMPORT_NETWORK,\n",
    "    \"network_class_name\": NETWORK_CLASS_NAME,\n",
    "    \"n_epochs\":N_EPOCHS,\n",
    "    \"batch_size\":BATCH_SIZE,\n",
    "    \"lr\":LR,\n",
    "    \"mean_error\":mean_error_rad,\n",
    "    }\n",
    "torch.save(network.state_dict(), PATH_TO_MODEL)\n",
    "with open(PATH_TO_MODEL_INFO,\"w\") as f:\n",
    "    json.dump(info_about_training,f)\n",
    "print(PATH_TO_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
